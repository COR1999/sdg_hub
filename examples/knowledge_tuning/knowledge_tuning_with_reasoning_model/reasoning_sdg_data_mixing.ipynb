{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Summary\n",
    "\n",
    "This notebook demonstrates how to the training dataset to train a student model. This notebook continues where left before. Recall in the reasoning notebook we saw how to modify current knowledge flow, swap a teacher model with reasoning model, and generate reasoning synthetic data. This notebook will focus on **data mixing and replay buffer strategies** to enhance the diversity and quality of training data. The notebook shows how to:\n",
    "\n",
    "* Show you how to create training mix using generated data and existing released instruct data\n",
    "* How instructlab data mixing for knowledge works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Nemotron Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "nemotron_ds = load_dataset(\"nvidia/Llama-Nemotron-Post-Training-Dataset-v1\", \"SFT\")\n",
    "nemotron_ds = nemotron_ds.filter(lambda x: x['used_in_training'] == 'yes')\n",
    "nemotron_ds = nemotron_ds.map(lambda x: {'question': x['input'][x['input'].find(\"user<|end_header_id|>\") + len(\"user<|end_header_id|>\") : x['input'].find(\"<|eot_id|><|start_header_id|>assistant\")].strip()})\n",
    "nemotron_ds = concatenate_datasets(nemotron_ds.values())\n",
    "nemotron_ds = nemotron_ds.shuffle(seed=894375).select(range(200000))\n",
    "nemotron_ds = nemotron_ds.add_column('unmask', [False]*nemotron_ds.num_rows)\n",
    "nemotron_ds = nemotron_ds.map(lambda x: {'messages': [{'role': 'system', 'content': 'detailed thinking on'}, {'role': 'user', 'content': x['question']}, {'role': 'assistant', 'content': x['output']}]})\n",
    "nemotron_ds = nemotron_ds.remove_columns(['input', 'output', 'category', 'license', 'reasoning', 'generator', 'used_in_training', 'question'])\n",
    "nemotron_ds.to_json(\"nemotron_replay_buffer_data.jsonl\", orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create functions for data mixng with conversation templates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section uses utility functions for creating training data that combines generated reasoning examples.\n",
    "The functions handle tasks like:\n",
    "- Converting documents and Q&A pairs into chat format\n",
    "- Adding system prompts for controlling model behavior\n",
    "- Mixing in auxiliary data like summaries\n",
    "\n",
    "These functions are similar to instructlab.sdg's data mixing functions\n",
    "\n",
    "The core functions are defined in:\n",
    "- `sdg_hub/examples/reasoning_knowledge_generation/utils.py`\n",
    "- `sdg_hub/examples/knowledge_tuning/knowledge_utils.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import json\n",
    "import uuid\n",
    "import os\n",
    "import sdg_hub\n",
    "import yaml\n",
    "import random\n",
    "import sys\n",
    "sys.path.insert(0, os.path.dirname(os.path.join(os.getcwd())))\n",
    "from knowledge_utils import generate_knowledge_qa_dataset, _conv_pretrain, create_summary_task_dataset\n",
    "\n",
    "    \n",
    "def create_training_mix(ds, tokenizer, thinking=\"on\", create_summary=True, nemotron_format=True, keep_context_separate=False, no_pretrain=False, keep_document_outline=False):\n",
    "    \"\"\"\n",
    "    Create a mixed training dataset combining knowledge QA and optional summary data.\n",
    "    \n",
    "    Args:\n",
    "        ds (Dataset): Input dataset\n",
    "        tokenizer: Tokenizer for pretraining format\n",
    "        thinking (str): Thinking mode for system message (\"on\"/\"off\")\n",
    "        create_summary (bool): Whether to include summary dataset\n",
    "        nemotron_format (bool): Whether to add system messages in nemotron format\n",
    "        keep_context_separate (bool): Whether to keep context separate in knowledge QA\n",
    "        no_pretrain (bool): Skip pretraining format conversion if True\n",
    "        keep_document_outline (bool): Include document outline in messages\n",
    "        \n",
    "    Returns:\n",
    "        Dataset: Combined training dataset\n",
    "    \"\"\"\n",
    "    # Generate knowledge QA dataset\n",
    "    knowl_train = generate_knowledge_qa_dataset(ds, keep_context_separate=keep_context_separate, keep_document_outline=keep_document_outline)\n",
    "    \n",
    "    # Apply pretraining format if needed\n",
    "    if no_pretrain:\n",
    "        knowl_train_pretrain = knowl_train\n",
    "    else:\n",
    "        knowl_train_pretrain = knowl_train.map(_conv_pretrain,  num_proc=10)\n",
    "    \n",
    "    # Add system messages for nemotron format\n",
    "    if nemotron_format:\n",
    "        knowl_train_pretrain = knowl_train_pretrain.map(lambda x: {'messages': [{'content': f'detailed thinking {thinking}', 'role': 'system'}] + x['messages']})\n",
    "    \n",
    "    # Add summary dataset if requested\n",
    "    if create_summary:\n",
    "        summary_ds = create_summary_task_dataset(ds)\n",
    "        if no_pretrain and summary_ds:\n",
    "            summary_ds_pretrain = summary_ds\n",
    "        else:\n",
    "            summary_ds_pretrain = summary_ds.map(_conv_pretrain, num_proc=10)\n",
    "        if nemotron_format:\n",
    "            summary_ds_pretrain = summary_ds_pretrain.map(lambda x: {'messages': [{'content': 'detailed thinking off', 'role': 'system'}] + x['messages']})\n",
    "        return concatenate_datasets([knowl_train_pretrain, summary_ds_pretrain])\n",
    "    else:\n",
    "        return knowl_train_pretrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create quality training mix of: reasoning dataset, with non-reasoning dataset, nemotron replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### For this tutorial, we will use the following document uids from the quality dataset:\n",
    "DOC_UIDS = [\n",
    "    ' Defining Decay Down by David Plotz',\n",
    "    ' Fight Clubbed by David Plotz',\n",
    "    ' I, Antichrist? by Jeffrey Goldberg',\n",
    "    \" It's Time To Keelhaul U-Haul! by Jeffrey Goldberg\",\n",
    "    \" My Father's Estate by Ben Stein\",\n",
    "    '\"Phone Me in Central Park\" by McConnell, James V.',\n",
    "    '...After a Few Words... by Garrett, Randall', \n",
    "    '...And It Comes Out Here by Del Rey, Lester',\n",
    "    'A Coffin for Jacob by Ludwig, Edward W.',\n",
    "    'A Fall of Glass by Lee, Stanley R.',\n",
    "    'A Filbert Is a Nut by Raphael, Rick',\n",
    "    'A Gift from Earth by Banister, Manly',\n",
    "    'A Gleeb for Earth by Schafhauser, Charles',\n",
    "    'A Good Year for the Roses? by David Edelstein',\n",
    "    'A Pail of Air by Leiber, Fritz',\n",
    "    'A Planet Named Joe by Hunter, Evan',\n",
    "    \"AI: what's the worst that could happen? by Harry Armstrong\",\n",
    "    'Accidental Death by Baily, Peter',\n",
    "    'All Day September by Kuykendall, Roger',\n",
    "    'Ambition by Bade, William L.',\n",
    "    'And Then the Town Took Off by Wilson, Richard',\n",
    "    'Atom Mystery [Young Atom Detective] by Coombs, Charles Ira',\n",
    "    'Beach Scene by King, Marshall',\n",
    "    'Big Ancestor by Wallace, F. L. (Floyd L.)',\n",
    "    'Birds of a Feather by Silverberg, Robert',\n",
    "    'Bodyguard by Gold, H. L. (Horace Leonard)'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    " \n",
    "# Load tokenizer for pre-training formatting\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nvidia/Llama-3.1-Nemotron-Nano-8B-v1\")\n",
    "\n",
    "# Load non-reasoning dataset from nemotron super 49b\n",
    "nemotron_non_reasoning_ds = load_dataset(\"json\", data_dir=\"data/knowledge/quality/knowledge_nemotron/\", split=\"train\")\n",
    "nemotron_non_reasoning_ds = nemotron_non_reasoning_ds.filter(lambda x: x['score'] == '2' and x['judgment'] == 'YES')\n",
    "nemotron_non_reasoning_ds = nemotron_non_reasoning_ds.filter(lambda x: x['document_outline'] in DOC_UIDS)\n",
    "print(nemotron_non_reasoning_ds)\n",
    "\n",
    "# Load reasoning dataset from nemotron super 49b\n",
    "nemotron_reasoning_ds = load_dataset(\"json\", data_dir=\"data/knowledge/quality/synth_knowledge_reasoning/\", split=\"train\")\n",
    "nemotron_reasoning_ds = nemotron_reasoning_ds.filter(lambda x: x['score'] == '2' and x['judgment'] == 'YES')\n",
    "nemotron_reasoning_ds = nemotron_reasoning_ds.filter(lambda x: x['document_outline'] in DOC_UIDS)\n",
    "print(nemotron_reasoning_ds)\n",
    "\n",
    "# Load nemotron replay buffer. \n",
    "# Note: This is a replay buffer we created by sub-sampling nvidia/Llama-Nemotron-Post-Training-Dataset from huggingface.\n",
    "nemotron_ds_replay_buffer = load_dataset(\"json\", data_files=\"data/knowledge/quality/training_mix/replay_buffer.jsonl\", split=\"train\")\n",
    "\n",
    "\n",
    "# Create non-reasoning training mix\n",
    "nemotron_ds_training_mix = create_training_mix(nemotron_non_reasoning_ds, tokenizer, 'off').shuffle(seed=894375)\n",
    "\n",
    "# Create reasoning training mix\n",
    "nemotron_reasoning_ds = create_training_mix(nemotron_reasoning_ds, tokenizer, 'on')\n",
    "\n",
    "# Concatenate reasoning and non-reasoning training mixes\n",
    "quality_reasoning_ds = concatenate_datasets([nemotron_reasoning_ds, nemotron_ds_training_mix]).remove_columns(['metadata', 'id']) # .select(range(40000))\n",
    "print(quality_reasoning_ds)\n",
    "\n",
    "# Concatenate training mix with replay buffer\n",
    "training_mix = concatenate_datasets([quality_reasoning_ds, nemotron_ds_replay_buffer.shuffle(seed=894375).select(range(len(quality_reasoning_ds)))])\n",
    "\n",
    "print(training_mix)\n",
    "training_mix.to_json(\"data/knowledge/quality/training_mix/quality_knowledge_mix.jsonl\", orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train student model\n",
    "- Setup the training by cloning `https://github.com/instructlab/training` and following the instructions in the README\n",
    "- The create `train.py` using below code\n",
    "    ```python\n",
    "    import argparse\n",
    "    from instructlab.training.config import TorchrunArgs,TrainingArgs,DistributedBackend,FSDPOptions\n",
    "    from instructlab.training.main_ds import run_training\n",
    "    import os\n",
    "    def parse_args():\n",
    "        parser = argparse.ArgumentParser(description='Training script with configurable paths')\n",
    "        parser.add_argument('--data_path', type=str, required=True,\n",
    "                        help='Path to the training data file')\n",
    "        parser.add_argument('--model_path', type=str, required=True,\n",
    "                        help='Path to the model or model identifier')\n",
    "        parser.add_argument('--chat_tmpl_path', type=str, required=True,\n",
    "                        help='Path to the chat template file')\n",
    "        parser.add_argument('--exp_dir', type=str, required=True,\n",
    "                        help='Path to the experiment directory')\n",
    "        parser.add_argument('--parent_exp_dir', type=str, required=True,\n",
    "                        help='Path to the parent experiment directory')\n",
    "        return parser.parse_args()\n",
    "\n",
    "    def main():\n",
    "        args = parse_args()\n",
    "        \n",
    "        torch_args = TorchrunArgs(\n",
    "            nproc_per_node=8,\n",
    "            nnodes=1,\n",
    "            node_rank=0,\n",
    "            rdzv_id=123,\n",
    "            rdzv_endpoint=\"0.0.0.0:8888\",\n",
    "        )\n",
    "        output_dir = os.path.join(args.parent_exp_dir, args.exp_dir)\n",
    "        train_args = TrainingArgs(\n",
    "            model_path=args.model_path,\n",
    "            data_path=args.data_path,\n",
    "            ckpt_output_dir=output_dir,\n",
    "            data_output_dir=\"data/processed-data\",\n",
    "            max_seq_len=20000,\n",
    "            max_batch_len=25000,\n",
    "            num_epochs=5,\n",
    "            effective_batch_size=256,\n",
    "            learning_rate=5e-6,\n",
    "            warmup_steps=25,\n",
    "            save_samples=0,\n",
    "            use_dolomite=False,\n",
    "            checkpoint_at_epoch = True,\n",
    "            accelerate_full_state_at_epoch = False,\n",
    "            process_data=True,\n",
    "            chat_tmpl_path=args.chat_tmpl_path,\n",
    "            distributed_backend=DistributedBackend.FSDP,\n",
    "            fsdp_options=FSDPOptions(cpu_offload_params=False),\n",
    "        )\n",
    "\n",
    "        run_training(torch_args=torch_args,train_args=train_args)\n",
    "\n",
    "    if __name__ == \"__main__\":\n",
    "        main()\n",
    "    ```\n",
    "\n",
    "- Now create bash script with run command\n",
    "\n",
    "    ```shell\n",
    "    python train.py \\\n",
    "    --data_path \"quality_knowledge_1.25_nemotron_49b_first_24.jsonl\" \\\n",
    "    --model_path \"nvidia/Llama-3.1-Nemotron-Nano-8B-v1\" \\\n",
    "    --chat_tmpl_path \"<chat_template_path>\" \\\n",
    "    --exp_dir \"nano_customized_thinking_quality_model\" \\\n",
    "    --parent_exp_dir \"<parent_exp_dir>\"\n",
    "    ```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research_sdg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
